{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLIP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: img/bathroom/IMG_1559.jpg\n",
      "Processed: img/bathroom/dscn2018.jpg\n",
      "Processed: img/bathroom/p8270261.jpg\n",
      "Processed: img/bathroom/IMG_3211.jpg\n",
      "Processed: img/bathroom/interior016.jpg\n",
      "Processed: img/bathroom/b8.jpg\n",
      "Processed: img/bathroom/indoor_0196.jpg\n",
      "Processed: img/bathroom/indoor_0226.jpg\n",
      "Processed: img/bathroom/indoor_0034.jpg\n",
      "Processed: img/bathroom/room508.jpg\n",
      "Processed: img/bathroom/IMG_1341.jpg\n",
      "Processed: img/bathroom/room471.jpg\n",
      "Processed: img/bathroom/room315.jpg\n",
      "Processed: img/bathroom/IMG_9647.jpg\n",
      "Processed: img/bathroom/IMG_2437.jpg\n",
      "Processed: img/bathroom/dublin___apartamento___29_03_2007_006.jpg\n",
      "Processed: img/bathroom/IMG_2439.jpg\n",
      "Processed: img/bathroom/room318.jpg\n",
      "Processed: img/bathroom/bathroom35.jpg\n",
      "Processed: img/bathroom/b12.jpg\n",
      "Processed: img/bathroom/n190011.jpg\n",
      "Processed: img/bathroom/indoor_0566.jpg\n",
      "Processed: img/bathroom/IMG_0076.jpg\n",
      "Processed: img/bathroom/indoor_0124.jpg\n",
      "Processed: img/bathroom/indoor_0391.jpg\n",
      "Processed: img/bathroom/img_0018.jpg\n",
      "Processed: img/bathroom/int480.jpg\n",
      "Processed: img/bathroom/100_0281.jpg\n",
      "Processed: img/bathroom/IMG_1084.jpg\n",
      "Processed: img/bathroom/IMG_5785.jpg\n",
      "Processed: img/bathroom/dsc01782a.jpg\n",
      "Processed: img/bathroom/IMG_3735.jpg\n",
      "Processed: img/bathroom/room29.jpg\n",
      "Processed: img/bathroom/IMG_1701.jpg\n",
      "Processed: img/bathroom/indoor_0461.jpg\n",
      "Processed: img/bathroom/IMG_3272.jpg\n",
      "Processed: img/bathroom/IMG_3270.jpg\n",
      "Processed: img/bathroom/IMG_4832.jpg\n",
      "Processed: img/bathroom/b4.jpg\n",
      "Processed: img/bathroom/b3.jpg\n",
      "Processed: img/bathroom/IMG_9648.jpg\n",
      "Processed: img/bathroom/b6.jpg\n",
      "Processed: img/bathroom/indoor_0493.jpg\n",
      "Processed: img/bathroom/IMG_2332.jpg\n",
      "Processed: img/bathroom/b9.jpg\n",
      "Processed: img/bathroom/indoor_0187.jpg\n",
      "Processed: img/bathroom/img_0029.jpg\n",
      "Processed: img/bathroom/room319.jpg\n",
      "Processed: img/bathroom/img_2399.jpg\n",
      "Processed: img/bathroom/room268.jpg\n",
      "Processed: img/bathroom/indoor_0309.jpg\n",
      "Processed: img/bathroom/indoor_0197.jpg\n",
      "Processed: img/bathroom/indoor_0489.jpg\n",
      "Processed: img/bathroom/indoor_0078.jpg\n",
      "Processed: img/bathroom/IMG_2334.jpg\n",
      "Processed: img/bathroom/indoor_0416.jpg\n",
      "Processed: img/bathroom/bath26.jpg\n",
      "Processed: img/bathroom/indoor_0363.jpg\n",
      "Processed: img/bathroom/indoor_0368.jpg\n",
      "Processed: img/bathroom/IMG_9819.jpg\n",
      "Processed: img/bathroom/indoor_0279.jpg\n",
      "Processed: img/bathroom/IMG_1252.jpg\n",
      "Processed: img/bathroom/img_2398.jpg\n",
      "Processed: img/bathroom/indoor_0332.jpg\n",
      "Processed: img/bathroom/b5.jpg\n",
      "Processed: img/bathroom/indoor_0242.jpg\n",
      "Processed: img/bathroom/indoor_0498.jpg\n",
      "Processed: img/bathroom/dsc3101.jpg\n",
      "Processed: img/bathroom/room299.jpg\n",
      "Processed: img/bathroom/b10.jpg\n",
      "Processed: img/bathroom/room483.jpg\n",
      "Processed: img/bathroom/IMG_2335.jpg\n",
      "Processed: img/bathroom/IMG_0999.jpg\n",
      "Processed: img/bathroom/indoor_0452.jpg\n",
      "Processed: img/bathroom/IMG_3024.jpg\n",
      "Processed: img/bathroom/room32.jpg\n",
      "Processed: img/bathroom/p1020935.jpg\n",
      "Processed: img/bathroom/IMG_4833.jpg\n",
      "Processed: img/bathroom/indoor_0323.jpg\n",
      "Processed: img/bathroom/room421.jpg\n",
      "Processed: img/bathroom/indoor_0286.jpg\n",
      "Processed: img/bathroom/indoor_0333.jpg\n",
      "Processed: img/bathroom/p8270262.jpg\n",
      "Processed: img/bathroom/interior015.jpg\n",
      "Processed: img/bathroom/int482.jpg\n",
      "Processed: img/bathroom/IMG_3736.jpg\n",
      "Processed: img/bathroom/b2.jpg\n",
      "Processed: img/bathroom/IMG_9818.jpg\n",
      "Processed: img/bathroom/room492.jpg\n",
      "Processed: img/bathroom/indoor_0486.jpg\n",
      "Processed: img/bathroom/int683.jpg\n",
      "Processed: img/bathroom/bath98.jpg\n",
      "Processed: img/bathroom/room270.jpg\n",
      "Processed: img/bathroom/indoor_0278.jpg\n",
      "Processed: img/bathroom/IMG_1340.jpg\n",
      "Processed: img/bathroom/IMG_1704.jpg\n",
      "Processed: img/bathroom/bathroom2.jpg\n",
      "Processed: img/bathroom/room432.jpg\n",
      "Processed: img/bathroom/b1.jpg\n",
      "Processed: img/bathroom/IMG_5748.jpg\n",
      "Processed: img/bathroom/ta_99_2_0319_02_l.jpg\n",
      "Processed: img/bathroom/indoor_0531.jpg\n",
      "Processed: img/bathroom/bothroom99.jpg\n",
      "Processed: img/bathroom/indoor_0491.jpg\n",
      "Processed: img/bathroom/IMG_2831.jpg\n",
      "Processed: img/bathroom/int17.jpg\n",
      "Processed: img/bathroom/IMG_0073.jpg\n",
      "Processed: img/bathroom/room317.jpg\n",
      "Processed: img/bathroom/cimg2046.jpg\n",
      "Processed: img/bathroom/room118.jpg\n",
      "Processed: img/bathroom/bath287.jpg\n",
      "Processed: img/bathroom/b11.jpg\n",
      "Processed: img/bathroom/bath244.jpg\n",
      "Processed: img/bathroom/indoor_0433.jpg\n",
      "Processed: img/bathroom/008.jpg\n",
      "Processed: img/bathroom/indoor_0399.jpg\n",
      "Processed: img/bathroom/indoor_0065.jpg\n",
      "Processed: img/bathroom/room280.jpg\n",
      "Processed: img/bathroom/indoor_0258.jpg\n",
      "Processed: img/bathroom/IMG_3701.jpg\n",
      "Processed: img/bathroom/indoor_0217.jpg\n",
      "Processed: img/bathroom/indoor_0342.jpg\n",
      "Processed: img/bathroom/indoor_0513.jpg\n",
      "Processed: img/bathroom/IMG_4834.jpg\n",
      "Processed: img/bathroom/cdmc1088.jpg\n",
      "Processed: img/bathroom/IMG_2438.jpg\n",
      "Processed: img/bathroom/bath181.jpg\n",
      "Processed: img/bathroom/IMG_1313.jpg\n",
      "Processed: img/bathroom/room357.jpg\n",
      "Processed: img/bathroom/bano3_2.jpg\n",
      "Processed: img/bathroom/room431.jpg\n",
      "Processed: img/bathroom/indoor_0458.jpg\n",
      "Processed: img/bathroom/im3.jpg\n",
      "Processed: img/bathroom/cdmc1117.jpg\n",
      "Processed: img/bathroom/indoor_0264.jpg\n",
      "Processed: img/bathroom/indoor_0082.jpg\n",
      "Processed: img/bathroom/pittsburgh_IMG_4080.jpg\n",
      "Processed: img/bathroom/img_0646.jpg\n",
      "Processed: img/bathroom/bath288.jpg\n",
      "Processed: img/bathroom/room30.jpg\n",
      "Processed: img/bathroom/room486.jpg\n",
      "Processed: img/bathroom/room322.jpg\n",
      "Processed: img/bathroom/n190015.jpg\n",
      "Processed: img/bathroom/room267.jpg\n",
      "Processed: img/bathroom/IMG_1085.jpg\n",
      "Processed: img/bathroom/img_1115.jpg\n",
      "Processed: img/bathroom/dsc01582.jpg\n",
      "Processed: img/bathroom/room482.jpg\n",
      "Processed: img/bathroom/indoor_0407.jpg\n",
      "Processed: img/bathroom/IMG_9646.jpg\n",
      "Processed: img/bathroom/bath166.jpg\n",
      "Processed: img/bathroom/room31.jpg\n",
      "Processed: img/bathroom/dsc00032.jpg\n",
      "Processed: img/bathroom/100_1412.jpg\n",
      "Processed: img/bathroom/p1010335.jpg\n",
      "Processed: img/bathroom/itoiletpaper.jpg\n",
      "Processed: img/bathroom/indoor_0404.jpg\n",
      "Processed: img/bathroom/n190010.jpg\n",
      "Processed: img/bathroom/n190042.jpg\n",
      "Processed: img/bathroom/cdmc1116.jpg\n",
      "Processed: img/bathroom/14_master_bathroom_jl.jpg\n",
      "Processed: img/bathroom/IMG_3018.jpg\n",
      "Processed: img/bathroom/IMG_5749.jpg\n",
      "Processed: img/bathroom/b7.jpg\n",
      "Processed: img/bathroom/indoor_0045.jpg\n",
      "Processed: img/bathroom/interior017.jpg\n",
      "Processed: img/bathroom/bath.jpg\n",
      "Processed: img/bathroom/room316.jpg\n",
      "Processed: img/bathroom/room311.jpg\n",
      "Processed: img/bathroom/bath31.jpg\n",
      "Processed: img/bathroom/indoor_0324.jpg\n",
      "Processed: img/bathroom/IMG_1005.jpg\n",
      "Processed: img/bathroom/IMG_0074.jpg\n",
      "Processed: img/bathroom/room484.jpg\n",
      "Processed: img/bathroom/roomscan34.jpg\n",
      "Processed: img/bathroom/room485.jpg\n",
      "Processed: img/bathroom/pasadena_IMG_0153.jpg\n",
      "Processed: img/bathroom/IMG_1560.jpg\n",
      "Processed: img/bathroom/pittsburgh_IMG_4083.jpg\n",
      "Processed: img/bathroom/IMG_1253.jpg\n",
      "Processed: img/bathroom/indoor_0135.jpg\n",
      "Processed: img/bathroom/indoor_0465.jpg\n",
      "Processed: img/bathroom/indoor_0432.jpg\n",
      "Processed: img/bathroom/int483.jpg\n",
      "Processed: img/bathroom/d47.jpg\n",
      "Processed: img/bathroom/bath17.jpg\n",
      "Processed: img/bathroom/IMG_3208.jpg\n",
      "Processed: img/bathroom/bath303.jpg\n",
      "Processed: img/bathroom/or_02_04_0211_08_l.jpg\n",
      "Processed: img/bathroom/pasadena_IMG_0154.jpg\n",
      "Processed: img/bathroom/indoor_0137.jpg\n",
      "Processed: img/bathroom/IMG_1343.jpg\n",
      "Processed: img/bathroom/IMG_1703.jpg\n",
      "Processed: img/bathroom/el_cuarto_de_bano.jpg\n",
      "Processed: img/bathroom/edificio_milano_ventas_alaria_034.jpg\n",
      "Processed: img/bathroom/IMG_5747.jpg\n",
      "Processed: img/bathroom/indoor_0364.jpg\n",
      "Descriptions saved to data/ground_truth-blip-image-captioning-base.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "        \"You are an advanced AI trained to generate highly detailed descriptions of images. \"\n",
    "        \"Use multi-step reasoning to identify all objects in the image, their relative positions, \"\n",
    "        \"and distances. Describe the scene as if explaining it to someone who cannot see it. \"\n",
    "        \"Be precise and include relevant spatial relationships.\"\n",
    "        # \"<image>\"\n",
    "    )\n",
    "\n",
    "MODEL_HF = \"Salesforce/blip-image-captioning-base\"\n",
    "CACHE_DIR = \"cache\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def generate_description(image_path, model, processor, device, question=SYSTEM_PROMPT):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, question=SYSTEM_PROMPT, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=100)\n",
    "    \n",
    "    description = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    return description.strip()\n",
    "\n",
    "def process_images(root_folder=\"img/bathroom\", output_csv=\"data/ground_truth-\"+MODEL_HF.split(\"/\")[1]+\".csv\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    processor = BlipProcessor.from_pretrained(MODEL_HF, cache_dir=CACHE_DIR)\n",
    "    model = BlipForConditionalGeneration.from_pretrained(MODEL_HF).to(device).eval()\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for subdir, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")):\n",
    "                image_path = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    description = generate_description(image_path, model, processor, device)\n",
    "                    data.append([image_path, description])\n",
    "                    print(f\"Processed: {image_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "    \n",
    "    with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"path\", \"description\"])\n",
    "        writer.writerows(data)\n",
    "    \n",
    "    print(f\"Descriptions saved to {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ground_truth_gen import load_model, save_results\n",
    "\n",
    "model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "processor, model = load_model(model_name, BlipProcessor, BlipForConditionalGeneration)  # Adjust if BLIP needs different processor/model classes\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"num_beams\": 5,\n",
    "    \"do_sample\": False\n",
    "}\n",
    "\n",
    "image_dir = \"img/test_blip\"\n",
    "output_csv = \"data/ground_truth_blip-base.csv\"\n",
    "\n",
    "results = process_images(image_dir, model, processor, generation_kwargs, output_csv)\n",
    "save_results(results, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLAVA-1.5-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "from transformers import LlavaProcessor, LlavaForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "MODEL_HF = \"llava-hf/llava-1.5-7b-hf\" # \"llava-hf/llava-1.5-7b-hf\"\n",
    "CACHE_DIR = \"cache\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def generate_description(image_path, model, processor):\n",
    "    system_prompt = (\n",
    "        \"<<SYS>>\\n\"\n",
    "        \"Please describe the image shown regarding spatial relationships between objects and their colors in great detail:\"\n",
    "        \"<image>\"\n",
    "        \"<<SYS>>\\n\"\n",
    "    )\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    device = model.device  # Get model's device\n",
    "    inputs = processor(images=image, text=system_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "    description = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "\n",
    "    if \"<<SYS>>\" in description:\n",
    "        description = description.split(\"<<SYS>>\")[-1].strip()\n",
    "\n",
    "    return description.strip()\n",
    "\n",
    "\n",
    "def process_images(root_folder=\"img/test\", output_csv=\"data/ground_truth-\"+MODEL_HF.split(\"/\")[1]+\".csv\"):\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(MODEL_HF, cache_dir=CACHE_DIR)\n",
    "    processor = LlavaProcessor.from_pretrained(MODEL_HF, cache_dir=CACHE_DIR)\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(MODEL_HF).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.eval()\n",
    "    data = []\n",
    "    \n",
    "    for subdir, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")):\n",
    "                image_path = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    description = generate_description(image_path, model, processor)\n",
    "                    data.append([image_path, description])\n",
    "                    print(f\"Processed: {image_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "    \n",
    "    with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"path\", \"description\"])\n",
    "        writer.writerows(data)\n",
    "    \n",
    "    print(f\"Descriptions saved to {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ground_truth_gen import load_model, save_results\n",
    "\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor, model = load_model(model_name, LlavaProcessor, LlavaForConditionalGeneration)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False\n",
    "}\n",
    "\n",
    "image_dir = \"img/test_llava\"\n",
    "output_csv = \"data/ground_truth_llava-1.5-7b-hf.csv\"\n",
    "\n",
    "results = process_images(image_dir, model, processor, generation_kwargs, output_csv)\n",
    "save_results(results, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaceLLAVA-13B &#9746;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import LlavaProcessor, LlavaForConditionalGeneration, AutoFeatureExtractor\n",
    "\n",
    "\n",
    "MODEL_HF = \"remyxai/SpaceLLaVA\"\n",
    "CACHE_DIR = \"cache\"\n",
    "\n",
    "def generate_description(image_path, model, processor):\n",
    "    system_prompt = (\n",
    "        \"<<SYS>>\\n\"\n",
    "        \"You are an advanced AI trained to generate highly detailed descriptions of images.\\n\"\n",
    "        \"Use multi-step reasoning to identify all objects in the image, their relative positions, \"\n",
    "        \"and distances. Describe the scene as if explaining it to someone who cannot see it.\\n\"\n",
    "        \"Be precise and include relevant spatial relationships.\\n\"\n",
    "        \"Describe the following image:\\n\"\n",
    "        \"<<SYS>>\\n\"\n",
    "        \"<image>\"\n",
    "    )\n",
    "\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_HF)\n",
    "\n",
    "    # Preprocess the image manually using the feature extractor\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    print(f\"Processed image with shape: {image_tensor.shape}\")\n",
    "    device = model.device  \n",
    "\n",
    "    # Correctly process image and text inputs\n",
    "    inputs = processor(images=image_tensor, text=system_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=256)\n",
    "\n",
    "    # Decode the generated description\n",
    "    description = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    return description.strip()\n",
    "\n",
    "def process_images(root_folder=\"img/test\", output_csv=\"data/ground_truth-\"+MODEL_HF.split(\"/\")[1]+\".csv\"):\n",
    "    # Load processor and model correctly\n",
    "    processor = LlavaProcessor.from_pretrained(MODEL_HF, cache_dir=CACHE_DIR)\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(MODEL_HF, cache_dir=CACHE_DIR).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model.eval()\n",
    "\n",
    "    data = []\n",
    "    for subdir, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")):\n",
    "                image_path = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    description = generate_description(image_path, model, processor)\n",
    "                    data.append([image_path, description])\n",
    "                    print(f\"Processed: {image_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "    with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"path\", \"description\"])\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"Descriptions saved to {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_images()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaceMantis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac8c9c9c07c4ecd914adf3208a3f842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New descriptions saved to data/ground_truth_SpaceMantis-8B.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from models.mllava import MLlavaProcessor, LlavaForConditionalGeneration, chat_mllava\n",
    "import pandas as pd\n",
    "\n",
    "CACHE_DIR = \"cache\"\n",
    "\n",
    "# Load the model and processor\n",
    "attn_implementation = None  # or \"flash_attention_2\"\n",
    "processor = MLlavaProcessor.from_pretrained(\"remyxai/SpaceMantis\", cache_dir=CACHE_DIR)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"remyxai/SpaceMantis\", cache_dir=CACHE_DIR, device_map=\"cuda\", torch_dtype=torch.float16, attn_implementation=attn_implementation)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False\n",
    "}\n",
    "\n",
    "# Function to run inference\n",
    "def run_inference(image_path, content):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # Convert the image to base64\n",
    "    images = [image]\n",
    "    # Run the inference\n",
    "    response, history = chat_mllava(content, images, model, processor, **generation_kwargs)\n",
    "    return response\n",
    "\n",
    "# Load the CSV file with ground truth descriptions\n",
    "csv_path = 'data/ground_truth-llava-1.5-7b-hf.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create a list to hold the new descriptions\n",
    "new_descriptions = []\n",
    "\n",
    "# Iterate through each row of the CSV to process the images and descriptions\n",
    "for index, row in df.iterrows():\n",
    "    image_path = row['path']\n",
    "    description = row['description']\n",
    "    \n",
    "    # Prepare the content for the model\n",
    "    content = f\"The following is a first version of the description: \\n\\n'{description}'.\\n\\n Modify the descriptions to add distances between objects and dimensions. Specify the metric you are using e.g. meters, feet.\"\n",
    "    #content=\"Please describe the image shown regarding spatial relationships between objects (specifying measures) in great detail\"\n",
    "    # Run inference on each image\n",
    "    response = run_inference(image_path, content)\n",
    "    \n",
    "    # Append the new description to the list\n",
    "    new_descriptions.append({\n",
    "        'path': image_path,\n",
    "        'description': response\n",
    "    })\n",
    "\n",
    "# Convert the list of new descriptions into a DataFrame\n",
    "new_df = pd.DataFrame(new_descriptions)\n",
    "\n",
    "# Save the new descriptions to a new CSV file\n",
    "new_csv_path = 'data/ground_truth_SpaceMantis-8B.csv'\n",
    "new_df.to_csv(new_csv_path, index=False)\n",
    "\n",
    "print(f\"New descriptions saved to {new_csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### W/ Multi-step reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from models.mllava import MLlavaProcessor, LlavaForConditionalGeneration, chat_mllava\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "CACHE_DIR = \"cache\"\n",
    "\n",
    "# Load the model and processor\n",
    "attn_implementation = None  # or \"flash_attention_2\"\n",
    "processor = MLlavaProcessor.from_pretrained(\"remyxai/SpaceMantis\", cache_dir=CACHE_DIR)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"remyxai/SpaceMantis\", cache_dir=CACHE_DIR, device_map=\"cuda\", torch_dtype=torch.float16, attn_implementation=attn_implementation)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False\n",
    "}\n",
    "\n",
    "# Function to run multi-step inference\n",
    "def run_inference(image_path):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    images = [image]\n",
    "\n",
    "    # Step 1: Basic Description\n",
    "    step1_prompt = \"Describe the image in great detail. Do not make anything up and do not assume anything. Only generate useful descriptive information.\"\n",
    "    step1_response, _ = chat_mllava(step1_prompt, images, model, processor, **generation_kwargs)\n",
    "\n",
    "    # Step 2: Identify Objects\n",
    "    step2_prompt = f\"Based on the previous description: \\n\\n'{step1_response}'.\\n\\n Now, modify it by listing all distinct objects in the image, specifying colors and positions in the image.\"\n",
    "    step2_response, _ = chat_mllava(step2_prompt, images, model, processor, **generation_kwargs)\n",
    "\n",
    "    # Step 3: Specify Semantic Relationships\n",
    "    step3_prompt = f\"Based on the identified objects: \\n\\n'{step2_response}'.\\n\\n Now, modify it by describing the semantic relationships between objects (e.g., one object is on top of another, next to, behind, etc.).\"\n",
    "    step3_response, _ = chat_mllava(step3_prompt, images, model, processor, **generation_kwargs)\n",
    "\n",
    "    # Step 4: Specify Exact Distances\n",
    "    step4_prompt = f\"Using the previous information: \\n\\n'{step3_response}'.\\n\\n Now, modify it by specifying the distances between objects in meters or feet.\"\n",
    "    step4_response, _ = chat_mllava(step4_prompt, images, model, processor, **generation_kwargs)\n",
    "\n",
    "    return step4_response\n",
    "\n",
    "# Load the CSV file with ground truth descriptions\n",
    "csv_path = 'data/ground_truth-llava-1.5-7b-hf.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Create a list to hold the new descriptions\n",
    "new_descriptions = []\n",
    "\n",
    "# Iterate through each row of the CSV to process the images and descriptions with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Images\", unit=\"image\"):\n",
    "    image_path = row['path']\n",
    "    \n",
    "    # Run inference on each image (multi-step reasoning)\n",
    "    response = run_inference(image_path)\n",
    "    \n",
    "    # Append the new description to the list\n",
    "    new_descriptions.append({\n",
    "        'path': image_path,\n",
    "        'description': response\n",
    "    })\n",
    "\n",
    "# Convert the list of new descriptions into a DataFrame\n",
    "new_df = pd.DataFrame(new_descriptions)\n",
    "\n",
    "# Save the new descriptions to a new CSV file\n",
    "new_csv_path = 'data/ground_truth_SpaceMantis-8B_4.csv'\n",
    "new_df.to_csv(new_csv_path, index=False)\n",
    "\n",
    "print(f\"New descriptions saved to {new_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ground_truth_gen import load_model, save_results\n",
    "\n",
    "model_name = \"remyxai/SpaceMantis\"\n",
    "processor, model = load_model(model_name, MLlavaProcessor, LlavaForConditionalGeneration)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"num_beams\": 1,\n",
    "    \"do_sample\": False\n",
    "}\n",
    "\n",
    "image_dir = \"img/test\" \n",
    "output_csv = \"data/ground_truth_SpaceMantis-8B_4.csv\"\n",
    "\n",
    "results = process_images(image_dir, model, processor, generation_kwargs)\n",
    "save_results(results, output_csv)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-17.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-17:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
