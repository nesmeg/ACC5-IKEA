{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup - Ollama"
      ],
      "metadata": {
        "id": "iwbLHm0-nn-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU gradio"
      ],
      "metadata": {
        "id": "0vZ3YXNxTUcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command installs or updates the Gradio library on your system. Gradio is a Python library that makes it easy to create web interfaces for machine learning models and data processing pipelines.\n",
        "\n",
        "The command has several parts:\n",
        "- `pip` is Python's package installer that downloads and sets up libraries\n",
        "- `install` tells pip to add a new package to your system\n",
        "- `-q` makes the installation \"quiet\" by hiding most of the technical output\n",
        "- `-U` forces an upgrade to the newest version if Gradio is already installed\n",
        "- `gradio` is the name of the package being installed\n",
        "\n",
        "When you run this, pip will silently download Gradio and all its required dependencies, then install or update them in your Python environment. The quiet flag helps keep your notebook or console clear of installation messages."
      ],
      "metadata": {
        "id": "FLXFm7weHXKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TERMINAL :\n",
        "\n",
        "# curl -fsSL https://ollama.com/install.sh | PATH=\"/sbin:/usr/sbin:$PATH\" sh\n",
        "\n",
        "# ollama serve &\n",
        "\n",
        "# ollama pull gemma2\n"
      ],
      "metadata": {
        "id": "oSyebNl292N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''\n",
        "curl -fsSL <https://ollama.com/install.sh> | sh\n",
        "'''\n",
        "\n",
        "This command downloads and runs the installation script for Ollama, a tool for running large language models locally. Let's break down each part to understand how it works:\n",
        "\n",
        "The command combines two main actions using the pipe operator (|). The first part downloads the script, and the second part executes it with specific path settings.\n",
        "\n",
        "In the first part, `curl -fsSL https://ollama.com/install.sh`:\n",
        "- `curl` is a tool that transfers data from or to a server\n",
        "- `-f` tells curl to fail silently on HTTP errors\n",
        "- `-s` runs curl in silent mode, hiding the progress bar\n",
        "- `-S` shows errors even in silent mode\n",
        "- `-L` makes curl follow redirects if the URL points to another location\n",
        "- The URL points to Ollama's installation script on their website\n",
        "\n",
        "The pipe operator `|` takes the downloaded script and sends it to the shell command that follows.\n",
        "\n",
        "In the second part, `PATH=\"/sbin:/usr/sbin:$PATH\" sh`:\n",
        "- `PATH=` temporarily modifies the system's PATH environment variable\n",
        "- `/sbin:/usr/sbin:$PATH` adds system administration directories to the front of the existing PATH\n",
        "- These directories contain important system utilities that the installation might need\n",
        "- `sh` runs the downloaded script using the shell with this modified PATH\n",
        "\n",
        "Adding `/sbin` and `/usr/sbin` to the PATH ensures the installation script can find all necessary system commands, even if they're not in the default user PATH. This makes the installation more reliable across different system configurations.\n",
        "\n",
        "When you run this command, it will download the installation script and execute it with elevated system access, setting up Ollama on your machine. The script runs with minimal output due to the silent flags, only showing important messages or errors."
      ],
      "metadata": {
        "id": "x7vVEfxP9s35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''\n",
        "ollama serve &\n",
        "'''\n",
        "This command starts the Ollama server as a background process. Let me explain what each part does and how they work together:\n",
        "\n",
        "The command has two main components: `ollama serve` and the `&` operator. Let's start by understanding the core function:\n",
        "\n",
        "`ollama serve` launches Ollama's server component, which is essential for running language models. When the server starts, it opens up an API endpoint (typically on port 11434) that allows other applications to communicate with and use the language models you've installed.\n",
        "\n",
        "The `&` at the end is a special shell operator that tells your system to run the process in the background. This means that after starting Ollama:\n",
        "1. Your terminal remains free for you to type other commands\n",
        "2. The server continues running even when you're doing other things\n",
        "3. You won't see the server's output directly in your terminal unless there's an error\n",
        "\n",
        "Think of this like starting a music player in the background - you want the music to keep playing while you do other things. Similarly, the Ollama server needs to keep running in the background so other applications can interact with it.\n",
        "\n",
        "One important detail to note is that since the process is running in the background, you won't immediately see if something goes wrong. You might want to check the server status using commands like `ps` or by trying to connect to it with a client application.\n",
        "\n",
        "If you later need to bring the process back to the foreground or stop it, you can use commands like `fg` to bring it to the foreground or `pkill ollama` to stop it completely."
      ],
      "metadata": {
        "id": "JQhrKbW5-B98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'''\n",
        "ollama pull gemma2\n",
        "'''\n",
        "\n",
        "This command downloads the Gemma 2 language model from Ollama's model repository and installs it on your system. Let me break down how this process works and what's happening behind the scenes.\n",
        "\n",
        "The command consists of two main parts: `ollama` is the base command that interacts with the Ollama system, and `pull gemma2` specifies that you want to download and install the Gemma 2 model. This is similar to how you might download an app from an app store - you're getting a pre-packaged piece of software that's ready to use.\n",
        "\n",
        "When you run this command, several things happen in sequence:\n",
        "1. Ollama first checks if you already have Gemma 2 installed locally\n",
        "2. If not, it connects to Ollama's model repository, which is like a library of available language models\n",
        "3. It begins downloading the model files, which can be quite large (often several gigabytes) since they contain all the neural network parameters that make the model work\n",
        "4. As it downloads, Ollama verifies the integrity of the files to ensure nothing was corrupted during transfer\n",
        "5. Finally, it sets up the model in your local Ollama installation so it's ready to use\n",
        "\n",
        "Gemma 2 is based on Google's Gemma architecture, which was designed to be more efficient than many other language models while maintaining strong performance. Think of it like getting a more fuel-efficient car that still has good acceleration and handling.\n",
        "\n",
        "Once the download completes, the model will be available for use through Ollama's API or command line interface. You'll be able to send it text prompts and receive generated responses, much like having a conversation with an AI assistant.\n",
        "\n",
        "If you want to use this model later, you can interact with it using commands like `ollama run gemma2`, which will start a conversation with the model in your terminal."
      ],
      "metadata": {
        "id": "FeN16ktE-MWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://localhost:11434/api/generate -d '{  \"model\": \"gemma2\",  \"prompt\":\"Why is the sky blue?\", \"stream\": false}'"
      ],
      "metadata": {
        "id": "ULgkseXH_pJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba96547-5c8f-45bd-a52b-5a811f4d4ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"model\":\"gemma2\",\"created_at\":\"2025-01-30T21:49:35.469651733Z\",\"response\":\"The sky appears blue due to a phenomenon called **Rayleigh scattering**. \\n\\nHere's a simplified explanation:\\n\\n1. **Sunlight:** Sunlight is actually made up of all the colors of the rainbow. When it enters Earth's atmosphere, it encounters tiny air molecules (mostly nitrogen and oxygen).\\n\\n2. **Scattering:** These air molecules scatter the sunlight in different directions.  \\n   - **Shorter wavelengths (blue and violet) are scattered more strongly** than longer wavelengths (red and orange). \\n\\n3. **Our Perception:** As a result, we see more blue light scattered throughout the sky than any other color. This makes the sky appear blue to our eyes.\\n\\n**Why not violet?** While violet light is scattered even more than blue, our eyes are less sensitive to violet.  Therefore, we perceive the sky as blue.\\n\\n\\nLet me know if you'd like a more detailed explanation or have any other questions!\",\"done\":true,\"done_reason\":\"stop\",\"context\":[106,1645,108,4385,603,573,8203,3868,235336,107,108,106,2516,108,651,8203,8149,3868,3402,577,476,27171,3151,5231,15599,44957,38497,168428,235248,109,4858,235303,235256,476,45138,15844,235292,109,235274,235265,5231,219715,66058,175521,603,4247,1644,908,576,832,573,9276,576,573,30088,235265,3194,665,30866,10379,235303,235256,13795,235269,665,56966,16791,2681,24582,591,80711,23584,578,16175,846,109,235284,235265,5231,102164,574,66058,3766,2681,24582,17109,573,33365,575,2167,16759,235265,139,108,140,235290,5231,3842,9733,95178,591,8796,578,45093,235275,708,30390,978,16066,688,1178,5543,95178,591,854,578,10436,846,235248,109,235304,235265,5231,5906,96569,66058,1877,476,2196,235269,783,1443,978,3868,2611,30390,6900,573,8203,1178,1089,1156,2881,235265,1417,3833,573,8203,4824,3868,577,1167,4628,235265,109,688,4385,780,45093,235336,688,7859,45093,2611,603,30390,1693,978,1178,3868,235269,1167,4628,708,2644,16955,577,45093,235265,139,24160,235269,783,44252,573,8203,685,3868,235265,110,5331,682,1230,1013,692,235303,235258,1154,476,978,11352,15844,689,791,1089,1156,3920,235341],\"total_duration\":15776441188,\"load_duration\":8435006338,\"prompt_eval_count\":15,\"prompt_eval_duration\":601000000,\"eval_count\":195,\"eval_duration\":6738000000}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command sends a request to Ollama's local API to generate text using the Gemma 2 model. Let's examine how this works by breaking down each component of the command.\n",
        "\n",
        "The core command `curl` is being used to send an HTTP POST request to `http://localhost:11434/api/generate`. The localhost address tells us this is connecting to a service running on your own computer - specifically the Ollama server we started earlier. Port 11434 is Ollama's default port for receiving API requests.\n",
        "\n",
        "The `-d` flag in curl indicates that we're sending data with our request. The data is formatted as a JSON object with three key pieces of information:\n",
        "1. `\"model\": \"gemma2\"` specifies which language model should process our request\n",
        "2. `\"prompt\": \"Why is the sky blue?\"` is the actual question we want the model to answer\n",
        "3. `\"stream\": false` tells Ollama to wait for the complete response before sending it back, rather than streaming the response word by word\n",
        "\n",
        "When you run this command, here's what happens in sequence:\n",
        "1. Your computer sends the request to the Ollama server running locally\n",
        "2. The server recognizes that you want to use the Gemma 2 model we downloaded earlier\n",
        "3. It loads the model if it isn't already loaded in memory\n",
        "4. The model processes the prompt about why the sky is blue\n",
        "5. Since streaming is disabled, Ollama waits for the model to finish generating its complete response\n",
        "6. The server sends back the finished response in a single JSON package\n",
        "\n",
        "This approach of using curl to interact with the API gives you fine-grained control over how you communicate with the model. It's like having a direct phone line to the AI - you can specify exactly what you want and how you want the response delivered.\n",
        "\n",
        "If you were building an application that uses this model, you might use similar API calls, but they would typically be wrapped in programming language-specific code rather than using curl directly. The API design makes it easy to integrate Ollama's capabilities into larger software systems."
      ],
      "metadata": {
        "id": "d8Os3XcUFV9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "7pDPQs6W_340"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "kQPDPaXu_-hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports two Python libraries that help create web applications:\n",
        "\n",
        "The first line brings in the 'requests' and 'json' libraries. The requests library lets the code make HTTP requests to other websites and servers - like fetching data from an API. The json library handles JSON data formats, which are commonly used when sending and receiving data over the internet.\n",
        "\n",
        "The second line imports the gradio library, giving it the shorter nickname 'gr'. Gradio is a framework that makes it simple to build web interfaces for Python code. It lets developers create user-friendly web apps where people can interact with machine learning models or other Python functions through buttons, text boxes, and other interface elements.\n",
        "\n",
        "Together, these imports set up the foundation for a web application that can communicate with external services and present an interface to users. The requests library will handle the external communication, json will process the data, and gradio will create the user interface."
      ],
      "metadata": {
        "id": "0J9mzBc9AhCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "  model = \"gemma2\""
      ],
      "metadata": {
        "id": "OLtpQD65AhSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting with `import requests, json`, we're bringing in two essential Python libraries:\n",
        "1. `requests` is a powerful library that lets our code communicate with web services, like making the same kinds of web requests a browser would make. Think of it as giving our program the ability to reach out and interact with other computers and services.\n",
        "2. `json` helps us work with JSON (JavaScript Object Notation) data - a format that's like a universal language for different computer systems to share information. When we send or receive data from web services, it often comes in JSON format.\n",
        "\n",
        "The second line `import gradio as gr` brings in the Gradio library, which we set up earlier with that pip install command. We're importing it with the nickname 'gr' to make it easier to reference. Gradio is a toolkit that helps us create user-friendly web interfaces for our Python code. It's particularly useful for making AI models accessible to people who might not be comfortable with code.\n",
        "\n",
        "By combining these imports, we're setting up the foundation to:\n",
        "1. Communicate with our Ollama server using requests\n",
        "2. Process the data we send and receive using json\n",
        "3. Create an interface that makes it easy for users to interact with our AI model using Gradio\n",
        "\n",
        "These three components work together like the pieces of a bridge - requests and json handle the backend communication with our AI model, while Gradio creates the frontend that users will actually see and interact with. This separation of concerns is a fundamental principle in software design, making our code both more organized and easier to maintain."
      ],
      "metadata": {
        "id": "eH-2JF7tG0aI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "Cr8kENUFAIbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, context, top_k, top_p, temp):\n",
        "    r = requests.post('http://localhost:11434/api/generate',\n",
        "                     json={\n",
        "                         'model': CFG.model, 'prompt': prompt,\n",
        "                         'context': context,\n",
        "                         'options':{\n",
        "                             'top_k': top_k,\n",
        "                             'temperature':top_p,\n",
        "                             'top_p': temp\n",
        "                         } },\n",
        "                     stream=False)\n",
        "    r.raise_for_status()\n",
        "\n",
        "    response = \"\"\n",
        "\n",
        "    for line in r.iter_lines():\n",
        "        body = json.loads(line)\n",
        "        response_part = body.get('response', '')\n",
        "        print(response_part)\n",
        "        if 'error' in body:\n",
        "            raise Exception(body['error'])\n",
        "\n",
        "        response += response_part\n",
        "\n",
        "        if body.get('done', False):\n",
        "            context = body.get('context', [])\n",
        "            return response, context"
      ],
      "metadata": {
        "id": "_WGeLatsAJRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me explain how this function works - it's handling communication with our AI model and managing the response in a structured way.\n",
        "\n",
        "The function `generate` takes five parameters that control how the AI generates text:\n",
        "- `prompt`: The text we want the AI to respond to\n",
        "- `context`: Previous conversation history that helps maintain coherent back-and-forth\n",
        "- `top_k`: Controls how many highest-probability words the model considers at each step\n",
        "- `top_p`: The nucleus sampling parameter that helps balance creativity and coherence\n",
        "- `temp`: Temperature setting that affects how random or focused the responses are\n",
        "\n",
        "When this function runs, it first sends a POST request to our local Ollama server. Think of this like dropping a letter in a mailbox - we're sending our request to a specific address (localhost:11434/api/generate) with specific instructions enclosed. The request includes:\n",
        "1. The model name (stored in CFG.model)\n",
        "2. Our prompt text\n",
        "3. Any context from previous exchanges\n",
        "4. Generation options that fine-tune how the AI thinks\n",
        "\n",
        "The `stream=False` parameter tells the server to send back the complete response rather than streaming it word by word. It's like asking for the whole story at once instead of hearing it sentence by sentence.\n",
        "\n",
        "After sending the request, `r.raise_for_status()` checks if anything went wrong. If the server reports an error, this line will raise an exception - it's like checking if our letter was actually delivered successfully.\n",
        "\n",
        "The function then processes the response line by line. For each line:\n",
        "1. It converts the JSON response into a Python object using `json.loads(line)`\n",
        "2. Extracts any response text using `body.get('response', '')`\n",
        "3. Prints each piece of the response\n",
        "4. Checks for errors\n",
        "5. Builds up the complete response by adding each new piece\n",
        "\n",
        "If the server signals it's done (`body.get('done', False)`), the function wraps up by:\n",
        "1. Grabbing any updated context for future exchanges\n",
        "2. Returning both the complete response and this context\n",
        "\n",
        "This two-part return value (response and context) is crucial - it's like getting both the answer to your current question and notes that help the AI remember the conversation for next time. The context helps maintain a coherent dialogue across multiple exchanges, much like how humans remember previous parts of a conversation to give relevant responses.\n",
        "\n",
        "The error handling throughout the function helps ensure we know if something goes wrong, making the system more reliable and easier to debug when problems occur."
      ],
      "metadata": {
        "id": "iqX0tePeALQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(input, chat_history, top_k, top_p, temp):\n",
        "\n",
        "    chat_history = chat_history or []\n",
        "\n",
        "    global context\n",
        "    output, context = generate(input, context, top_k, top_p, temp)\n",
        "\n",
        "    chat_history.append((input, output))\n",
        "\n",
        "    return chat_history, chat_history"
      ],
      "metadata": {
        "id": "0KM0U5nkALe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `chat` function takes five parameters:\n",
        "- `input`: The user's latest message\n",
        "- `chat_history`: A record of all previous exchanges\n",
        "- `top_k`, `top_p`, and `temp`: The same generation parameters we saw earlier that control how the AI responds\n",
        "\n",
        "Let's examine what happens step by step:\n",
        "\n",
        "First, the line `chat_history = chat_history or []` is a safety check. If `chat_history` is None or empty (which would be falsy in Python), it creates a new empty list. This ensures we always have a valid list to work with, even if this is the very first message in a conversation. Think of it like starting a new notebook if you don't already have one to write in.\n",
        "\n",
        "Next, we see `global context`. This tells Python we want to use and modify the `context` variable that exists outside this function. The context is like the AI's short-term memory of the conversation - it helps maintain coherent back-and-forth exchanges by remembering what was previously discussed.\n",
        "\n",
        "The line `output, context = generate(input, context, top_k, top_p, temp)` is where the magic happens. It:\n",
        "1. Calls our previous `generate` function with the new message and existing context\n",
        "2. Gets back both the AI's response and updated context\n",
        "3. Updates our global context with this new information\n",
        "\n",
        "Then, `chat_history.append((input, output))` adds the new exchange to our conversation record. It creates a tuple containing the user's input and the AI's output, like recording both sides of a dialogue. Each entry in chat_history captures one complete turn in the conversation.\n",
        "\n",
        "Finally, `return chat_history, chat_history` returns the updated conversation history twice. This might seem odd, but it's likely designed to work with Gradio's interface requirements, where multiple UI elements might need to display or use the chat history.\n",
        "\n",
        "The overall structure of this function creates a continuous conversation flow:\n",
        "1. It maintains the context needed for coherent exchanges\n",
        "2. Generates appropriate responses to each new input\n",
        "3. Keeps a record of the entire conversation\n",
        "4. Makes this history available for display or further processing\n",
        "\n",
        "This design allows for natural back-and-forth conversation while maintaining the context and history needed for meaningful exchanges. It's like having a secretary who not only helps you communicate but also keeps perfect records of every exchange."
      ],
      "metadata": {
        "id": "SVsDev_0AQvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "4j0fwm39ARaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = []"
      ],
      "metadata": {
        "id": "uDi36Q8JAfWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block = gr.Blocks()\n",
        "\n",
        "with block:\n",
        "\n",
        "    gr.Markdown(\"\"\"<h1><center> Muh private chatbot </center></h1>\n",
        "    \"\"\")\n",
        "\n",
        "    message = gr.Textbox(placeholder=\"Type here\")\n",
        "    chatbot = gr.Chatbot()\n",
        "\n",
        "\n",
        "    state = gr.State()\n",
        "\n",
        "    with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "\n",
        "      with gr.Row():\n",
        "          top_k = gr.Slider(0.0,100.0, label=\"top_k\", value=40, info=\"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\")\n",
        "          top_p = gr.Slider(0.0,1.0, label=\"top_p\", value=0.9, info=\" Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\")\n",
        "          temp = gr.Slider(0.0,2.0, label=\"temperature\", value=0.8, info=\"The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)\")\n",
        "\n",
        "\n",
        "    submit = gr.Button(\"SEND\")\n",
        "\n",
        "    submit.click(chat, inputs=[message, state, top_k, top_p, temp], outputs=[chatbot, state])\n",
        "\n",
        "block.launch(debug=True, share = True)"
      ],
      "metadata": {
        "id": "536yM8TpASEu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd8d9b2a-09c6-4671-c304-d553cacfc1cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://89e1c16b07d264420a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://89e1c16b07d264420a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            " am\n",
            " Gemma\n",
            ",\n",
            " an\n",
            " open\n",
            "-\n",
            "weights\n",
            " AI\n",
            " assistant\n",
            ".\n",
            " I\n",
            "'\n",
            "m\n",
            " a\n",
            " large\n",
            " language\n",
            " model\n",
            " trained\n",
            " by\n",
            " Google\n",
            " Deep\n",
            "Mind\n",
            ".\n",
            " My\n",
            " purpose\n",
            " is\n",
            " to\n",
            " help\n",
            " people\n",
            " by\n",
            " understanding\n",
            " and\n",
            " responding\n",
            " to\n",
            " their\n",
            " requests\n",
            " in\n",
            " a\n",
            " helpful\n",
            ",\n",
            " informative\n",
            ",\n",
            " and\n",
            " impartial\n",
            " way\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "Here\n",
            " are\n",
            " some\n",
            " key\n",
            " things\n",
            " to\n",
            " know\n",
            " about\n",
            " me\n",
            ":\n",
            "\n",
            "\n",
            "\n",
            "*\n",
            " **\n",
            "Open\n",
            "-\n",
            "Weights\n",
            ":**\n",
            " My\n",
            " weights\n",
            " are\n",
            " publicly\n",
            " available\n",
            ",\n",
            " meaning\n",
            " anyone\n",
            " can\n",
            " access\n",
            " and\n",
            " study\n",
            " them\n",
            ".\n",
            "\n",
            "\n",
            "*\n",
            " **\n",
            "Text\n",
            "-\n",
            "Only\n",
            ":**\n",
            " I\n",
            " can\n",
            " only\n",
            " communicate\n",
            " through\n",
            " text\n",
            ".\n",
            " I\n",
            " can\n",
            "'\n",
            "t\n",
            " generate\n",
            " images\n",
            ",\n",
            " sound\n",
            ",\n",
            " or\n",
            " videos\n",
            ".\n",
            "\n",
            "\n",
            "*\n",
            " **\n",
            "Limited\n",
            " Knowledge\n",
            ":**\n",
            " I\n",
            " don\n",
            "'\n",
            "t\n",
            " have\n",
            " access\n",
            " to\n",
            " real\n",
            "-\n",
            "time\n",
            " information\n",
            " or\n",
            " the\n",
            " internet\n",
            ".\n",
            " My\n",
            " knowledge\n",
            " is\n",
            " based\n",
            " on\n",
            " the\n",
            " data\n",
            " I\n",
            " was\n",
            " trained\n",
            " on\n",
            ",\n",
            " which\n",
            " has\n",
            " a\n",
            " cutoff\n",
            " point\n",
            ".\n",
            "\n",
            "\n",
            "*\n",
            " **\n",
            "Created\n",
            " by\n",
            " the\n",
            " Gemma\n",
            " Team\n",
            ":**\n",
            " I\n",
            " was\n",
            " developed\n",
            " by\n",
            " a\n",
            " team\n",
            " of\n",
            " engineers\n",
            " and\n",
            " researchers\n",
            " at\n",
            " Google\n",
            " Deep\n",
            "Mind\n",
            ".\n",
            "\n",
            "\n",
            "\n",
            "I\n",
            "'\n",
            "m\n",
            " always\n",
            " learning\n",
            " and\n",
            " improving\n",
            ",\n",
            " and\n",
            " I\n",
            "'\n",
            "m\n",
            " excited\n",
            " to\n",
            " see\n",
            " how\n",
            " people\n",
            " use\n",
            " me\n",
            " to\n",
            " explore\n",
            " new\n",
            " ideas\n",
            " and\n",
            " accomplish\n",
            " their\n",
            " goals\n",
            ".\n",
            "\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://89e1c16b07d264420a.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates an interactive web interface for a chatbot using Gradio. Let's break it down step by step.\n",
        "\n",
        "First, we create a container for our interface using gr.Blocks() and store it in the variable 'block'. Think of this like creating a blank canvas for our web page.\n",
        "\n",
        "Inside this container, we set up several interface elements:\n",
        "\n",
        "The gr.Markdown creates a centered heading that says \"Muh private chatbot\" at the top of the page. The HTML tags control the size and positioning.\n",
        "\n",
        "Next, we add two key components for the chat interaction:\n",
        "- A textbox (gr.Textbox) where users can type their messages, with a helpful \"Type here\" placeholder\n",
        "- A chatbot interface (gr.Chatbot) that will display the back-and-forth conversation\n",
        "\n",
        "The gr.State() creates a hidden component that maintains the conversation's state - like keeping track of the chat history between messages.\n",
        "\n",
        "The interface includes an advanced settings section, hidden by default (open=False), that users can expand. Inside this accordion menu, we have three sliders arranged in a row:\n",
        "\n",
        "1. top_k: Controls how selective the model is when choosing words. At 100, it considers many options, while at 10 it sticks to the most likely choices.\n",
        "2. top_p: Works with top_k to control text diversity. Higher values (like 0.95) encourage more varied responses, while lower values (like 0.5) keep responses more focused.\n",
        "3. temperature: Affects the model's creativity. Higher temperatures (up to 2.0) make responses more imaginative, while lower values make them more predictable.\n",
        "\n",
        "At the bottom, there's a \"SEND\" button. When clicked, it triggers the chat function (not shown in this code) with five inputs: the message, conversation state, and the three slider values. The function's results update both the chatbot display and the conversation state.\n",
        "\n",
        "Finally, block.launch() starts up the web interface with debugging enabled (debug=True) and makes it accessible to others (share=True).\n",
        "\n",
        "The structure flows naturally from top to bottom - from the header, to the main chat interface, to advanced settings, to the send button - creating an intuitive user experience where all controls are easily accessible."
      ],
      "metadata": {
        "id": "j-dCsaPCAYjp"
      }
    }
  ]
}