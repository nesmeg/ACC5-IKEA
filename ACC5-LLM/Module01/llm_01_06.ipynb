{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Xc_z0XnVI8yX",
        "UKee7vthNL9a"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b56cd47f561442ce8804ef96ef3a11da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf297d87e3c4438cacaf69045cfce7cf",
              "IPY_MODEL_111487eabb8d47fd836770f87677aa05",
              "IPY_MODEL_a960aee7cfe749fc843ad193b472ffd3"
            ],
            "layout": "IPY_MODEL_217c1f756fa54baaa997e81d5a156bef"
          }
        },
        "cf297d87e3c4438cacaf69045cfce7cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b77f3f9076241ccb9d81e6444251ee1",
            "placeholder": "​",
            "style": "IPY_MODEL_533f5512c79f4ba98533c030a2758530",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "111487eabb8d47fd836770f87677aa05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41ab047a863549dfb25945df889f2719",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dda19ad57d994b6fb2759add5ab619ad",
            "value": 2
          }
        },
        "a960aee7cfe749fc843ad193b472ffd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_343d84d8727341709700d96b31af2917",
            "placeholder": "​",
            "style": "IPY_MODEL_0abafeab9e9940869b1feb66dd8c7500",
            "value": " 2/2 [00:59&lt;00:00, 27.58s/it]"
          }
        },
        "217c1f756fa54baaa997e81d5a156bef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b77f3f9076241ccb9d81e6444251ee1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "533f5512c79f4ba98533c030a2758530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41ab047a863549dfb25945df889f2719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dda19ad57d994b6fb2759add5ab619ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "343d84d8727341709700d96b31af2917": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0abafeab9e9940869b1feb66dd8c7500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Xc_z0XnVI8yX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBuEgUU6I76S"
      },
      "outputs": [],
      "source": [
        "!pip install -qU bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command installs or upgrades the \"bitsandbytes\" Python package silently. The `-q` flag makes the installation quiet by suppressing most output messages, while `-U` ensures you get the latest version even if an older one exists.\n",
        "\n",
        "Bitsandbytes is a library that helps run large AI models more efficiently by using 8-bit quantization - a technique that reduces how much memory the models need. It's particularly useful when working with large language models or other deep learning projects where memory usage is a concern.\n",
        "\n",
        "The exclamation mark at the start tells Jupyter notebooks or Google Colab to run this as a shell command rather than Python code. Without it, you'd need to use Python's package management tools directly.\n",
        "\n",
        "The installation is handled by pip, which is Python's standard package installer. The equal relationship between pip and Python is similar to how npm works with JavaScript or cargo with Rust - it's the main way to add external code to your project."
      ],
      "metadata": {
        "id": "zBt_uoWhJBZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Core ML framework\n",
        "import torch\n",
        "\n",
        "# Torch backend configurations\n",
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)\n",
        "\n",
        "# Authentication and API clients\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Transformer-specific imports\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TextStreamer\n",
        ")\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "vBNmicHbJBm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up the infrastructure needed to work with large language models, particularly focusing on efficient memory usage and proper configuration. Let me break it down into logical sections:\n",
        "\n",
        "At the core, the code imports essential Python modules. The os module gives access to operating system functions like environment variables, while warnings lets us control which warning messages appear during execution.\n",
        "\n",
        "The code then configures PyTorch, the deep learning framework. It specifically disables two memory optimization features: memory-efficient scaled dot product attention and flash attention. These features can sometimes conflict with certain model configurations, so turning them off ensures compatibility.\n",
        "\n",
        "For working with models, the code brings in several key components from the transformers library. AutoModelForCausalLM handles text generation models - think of it as the engine that powers text completion. AutoTokenizer converts text into a format the model can understand, similar to how a translator converts between languages. BitsAndBytesConfig manages memory-efficient model loading, while TextStreamer helps display model output smoothly.\n",
        "\n",
        "The code sets up authentication too. It imports login from huggingface_hub to access models from Hugging Face's repository, and userdata from Google Colab to handle credentials. This combination allows secure access to hosted models.\n",
        "\n",
        "An important environment variable is set: HF_HUB_ENABLE_HF_TRANSFER = \"1\". This enables Hugging Face's optimized file transfer protocol, making model downloads more reliable and efficient.\n",
        "\n",
        "Finally, the code suppresses warnings using warnings.filterwarnings(\"ignore\"). While this makes the output cleaner, it's worth noting that in development environments, seeing these warnings can help catch potential issues early.\n",
        "\n",
        "The structure follows a common pattern in machine learning code: first setting up basic Python tools, then configuring the deep learning framework, and finally importing specialized components for model handling. Each import and configuration serves a specific purpose in creating a stable environment for working with large language models."
      ],
      "metadata": {
        "id": "mVthSRaeJLNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "login(token = userdata.get('HF_TOKEN') )"
      ],
      "metadata": {
        "id": "aDMa2_vPM7tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code handles authentication with the Hugging Face model hub. Let's break down what's happening and why it matters.\n",
        "\n",
        "The login function is attempting to authenticate with Hugging Face using a token stored in Google Colab's secure storage system. Think of this like using a special key card to enter a secure building - the token proves you have permission to access Hugging Face's resources.\n",
        "\n",
        "The userdata.get('HF_TOKEN') part is reaching into Colab's secure storage to retrieve your Hugging Face access token. This is similar to how a password manager securely stores your credentials. Storing the token this way, rather than writing it directly in the code, is a security best practice - it keeps sensitive credentials out of your code where they could be accidentally shared or exposed.\n",
        "\n",
        "The entire login() function call establishes a secure connection that will let you download models, push changes, or interact with private repositories on Hugging Face. Without this authentication step, you'd only have access to public models, or in some cases, no access at all.\n",
        "\n",
        "A key security feature here is that the token is being retrieved dynamically rather than hardcoded. This follows the principle of keeping sensitive credentials separate from code, similar to how websites store authentication tokens in secure HTTP-only cookies rather than in JavaScript code.\n",
        "\n",
        "If this login fails, it might mean either the token isn't properly stored in Colab's userdata, or the token itself might be invalid or expired. In that case, you'd need to generate a new token from your Hugging Face account settings and store it in Colab's secrets management system."
      ],
      "metadata": {
        "id": "J7B3ZlPPNE9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    device = 'cuda'\n",
        "    model = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "    max_tokens = 5000\n",
        "    temperature = 0.1\n",
        "    top_k = 5\n",
        "    top_p = 0.9\n",
        "    dtype = torch.bfloat16"
      ],
      "metadata": {
        "id": "MZLwJe71NFvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a configuration class that controls how a language model will run. Think of it like setting up the control panel before operating complex machinery - each setting affects how the model will behave.\n",
        "\n",
        "Let's examine each setting and understand its purpose:\n",
        "\n",
        "The device setting tells the model to run on a CUDA-enabled GPU rather than CPU. This is crucial for performance - GPUs can process the complex matrix operations in language models much faster than CPUs, similar to how a specialized wood-cutting machine works better than a general-purpose knife.\n",
        "\n",
        "The model selection specifies Phi-3-mini-4k-instruct from Microsoft. The \"4k\" in the name suggests it can handle contexts up to 4,000 tokens, while \"mini\" indicates it's a smaller, more efficient version. The \"instruct\" suffix means it's specifically tuned for following instructions, like a well-trained assistant.\n",
        "\n",
        "The max_tokens setting caps the length of generated text at 5,000 tokens. Think of this like setting a word limit on an essay - it prevents the model from rambling indefinitely.\n",
        "\n",
        "Temperature, top_k, and top_p work together to control the randomness and creativity of the model's outputs. With temperature at 0.1, the model will be quite deterministic, almost always choosing the most likely next word. This is like turning down the \"creativity dial\" to get more focused, predictable responses. The top_k value of 5 means it only considers the 5 most likely next words, while top_p of 0.9 ensures it captures 90% of the probability mass - these settings help balance between creativity and coherence.\n",
        "\n",
        "The dtype setting uses bfloat16, a special number format that saves memory while maintaining good numerical precision. This is like using efficient shorthand instead of writing everything in longhand - it helps the model fit in GPU memory while still performing well. The torch.bfloat16 format is particularly good for neural networks because it preserves more precision in larger numbers compared to regular 16-bit formats.\n",
        "\n",
        "Together, these settings create a configuration aimed at efficient, controlled text generation with a focus on instruction-following rather than creative exploration. The low temperature and limited top_k suggest this setup is designed for tasks requiring precise, consistent outputs."
      ],
      "metadata": {
        "id": "jug_pjl7NHcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funkcje"
      ],
      "metadata": {
        "id": "UKee7vthNL9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, system = None):\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    if system:\n",
        "        messages.append({\"role\": \"system\", \"content\": system})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "\n",
        "    tokenizer_output = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True)\n",
        "    model_input_ids = tokenizer_output.input_ids.to(CFG.device)\n",
        "    model_attention_mask = tokenizer_output.attention_mask.to(CFG.device)\n",
        "\n",
        "    outputs = model.generate(model_input_ids,\n",
        "                           attention_mask=model_attention_mask,\n",
        "                           streamer = streamer,\n",
        "                           max_new_tokens = CFG.max_tokens,\n",
        "                           do_sample=True if CFG.temperature else False,\n",
        "                           temperature = CFG.temperature,\n",
        "                           top_k = CFG.top_k,\n",
        "                           top_p = CFG.top_p)\n",
        "\n",
        "    answer = tokenizer.batch_decode(outputs, skip_special_tokens = False)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "XBZtKpBGNN-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function that generates text responses using a language model, similar to how a translator processes and generates text in different languages. Let's walk through how it works step by step.\n",
        "\n",
        "The function accepts two parameters: 'prompt' (the main text input) and 'system' (optional instructions that shape how the model should behave). Think of the prompt as the actual question you're asking, while the system parameter is like giving background instructions to a human assistant before they start their task.\n",
        "\n",
        "The function first creates a list called 'messages' to store the conversation structure. If there's a system message provided, it adds that first - like giving context before asking a question. Then it always adds the user's prompt. Each message is formatted as a dictionary with two keys: 'role' (who's speaking) and 'content' (what they're saying).\n",
        "\n",
        "Next comes the crucial preparation step where the text gets converted into a format the model can understand. The tokenizer.apply_chat_template method takes our messages and transforms them into numerical representations (tokens) that the model can process. This is similar to how a music sheet translates musical ideas into a standardized notation that musicians can read.\n",
        "\n",
        "The tokenized input gets moved to the specified device (GPU in this case) using .to(CFG.device). This is like moving your work materials to your workbench before starting a project. Both the input tokens (model_input_ids) and the attention mask (which tells the model which parts of the input to focus on) are transferred.\n",
        "\n",
        "The generation phase is where the model actually creates its response. The model.generate method takes several parameters that control how the text is generated:\n",
        "\n",
        "- streamer allows for real-time output display\n",
        "- max_new_tokens limits response length\n",
        "- do_sample determines if the model should use randomness in its choices (only active if temperature > 0)\n",
        "- temperature, top_k, and top_p control how creative versus deterministic the output should be\n",
        "\n",
        "Finally, the generated tokens are converted back into human-readable text using tokenizer.batch_decode. The skip_special_tokens parameter determines whether to include or remove special markers the model uses internally.\n",
        "\n",
        "Understanding this function is key because it encapsulates the entire process of how we interact with language models: taking human input, converting it to a form the model understands, generating a response under specific constraints, and converting that response back to human-readable text. It's like having a complete translation pipeline that handles both the conversion of languages and the generation of appropriate responses."
      ],
      "metadata": {
        "id": "jM8QbejvNOev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resolve_mistakes(text: str, options = ['all'], one_shot = True):\n",
        "    \"\"\"\n",
        "    A function that corrects mistakes in text using a language model.\n",
        "    The function specializes in different types of linguistic errors.\n",
        "    \"\"\"\n",
        "\n",
        "    system = \"You are an expert specializing in improving texts in English.\"\n",
        "\n",
        "    prompt_start = \"Check the following English text and correct it. Pay special attention to:\\n\\n\"\n",
        "\n",
        "    # Possible options to include as the second argument: \"typo\", \"spelling\", \"punctuation\", \"inflection\", \"syntax\", \"lexical\", \"stylistic\"\n",
        "    prompt_typo = \"* Typos (omission, repetition, or insertion of incorrect characters)\\n\\n\"\n",
        "    prompt_spelling = \"* Spelling mistakes (incorrect letter usage or combinations, incorrect capitalization, incorrect use of periods in abbreviations, incorrect joint writing)\\n\\n\"\n",
        "    prompt_punctuation = \"* Punctuation errors (unnecessary punctuation marks, missing punctuation marks, incorrect handling of multiple punctuation marks, use of incorrect punctuation marks)\\n\\n\"\n",
        "    prompt_inflection = \"* Inflection errors (related to choosing incorrect word forms, incorrect declension, or incorrect lack of declension)\\n\\n\"\n",
        "    prompt_syntax = \"* Syntax errors (using incorrect forms or constructions where required by governing words, word order errors, incorrect use of participle equivalents)\\n\\n\"\n",
        "    prompt_lexical = \"* Lexical errors (incorrect word usage in given constructions, confusing similar-sounding words, unnecessary borrowings or overuse of foreign words, word-formation errors and errors resulting from word-formation associations, distortions of phraseological units)\\n\\n\"\n",
        "    prompt_stylistic = \"* Stylistic errors (mismatched linguistic forms for correspondence character and function, unintentionally making text ambiguous)\\n\\n\"\n",
        "\n",
        "    prompt_one_shot = \"Here's an example: \\n\\nText to correct: 'pete sat hunced at the tble and was silent .'\\n\\nCorrectly fixed text: 'Pete sat hunched at the table and was silent.'\"\n",
        "    prompt_end = \"Remember that intervention in the content should only be related to error correction and should not interfere with the original meaning. Return only the corrected text without additional comments. Below is the text to correct:\\n\\n\"\n",
        "\n",
        "    if options[0] == 'all':\n",
        "        options = ['typo', 'spelling', 'punctuation', 'inflection', 'syntax', 'lexical', 'stylistic']\n",
        "\n",
        "    # Constructing the complete prompt by combining different error type prompts based on selected options\n",
        "    prompt_complete = prompt_start + (prompt_typo if 'typo' in options else '') + (prompt_spelling if 'spelling' in options else '') + (prompt_punctuation if 'punctuation' in options else '') + (prompt_inflection if 'inflection' in options else '') + (prompt_syntax if 'syntax' in options else '') + (prompt_lexical if 'lexical' in options else '') + (prompt_stylistic if 'stylistic' in options else '') + (prompt_one_shot if one_shot else '') + prompt_end + text\n",
        "\n",
        "    messages = []\n",
        "    messages.append({\"role\": \"system\", \"content\": system})\n",
        "    messages.append({\"role\": \"user\", \"content\": str(prompt_complete)})\n",
        "\n",
        "    # Processing the text through the language model\n",
        "    tokenizer_output = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True)\n",
        "    model_input_ids = tokenizer_output.input_ids.to(CFG.device)\n",
        "    model_attention_mask = tokenizer_output.attention_mask.to(CFG.device)\n",
        "\n",
        "    outputs = model.generate(model_input_ids,\n",
        "                           attention_mask=model_attention_mask,\n",
        "                           streamer = streamer,\n",
        "                           max_new_tokens=5000,\n",
        "                           do_sample=False,\n",
        "                           temperature = CFG.temperature, top_k = CFG.top_k, top_p = CFG.top_p)\n",
        "\n",
        "    answer = tokenizer.batch_decode(outputs, skip_special_tokens = False)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "XTjzQbiBNY2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function creates a sophisticated text correction system that can identify and fix various types of language errors. Let's explore how it works through each major component.\n",
        "\n",
        "The function starts by accepting three parameters: the text to correct, which error types to look for (defaulting to 'all'), and whether to include an example correction (one_shot). This flexibility allows users to target specific types of errors or conduct a comprehensive review.\n",
        "\n",
        "The heart of the function lies in its detailed prompt construction system. It builds specialized instructions for the language model by combining different components:\n",
        "\n",
        "First, it establishes the model's role through a system message that positions it as an English language expert. This creates a context where the model approaches the text with specialized linguistic knowledge.\n",
        "\n",
        "The prompt construction then uses a modular approach, breaking down potential errors into seven distinct categories, each targeting specific linguistic challenges:\n",
        "\n",
        "1. Typos focus on mechanical errors like missing or extra characters\n",
        "2. Spelling addresses systematic issues with how words are written\n",
        "3. Punctuation handles the intricate rules of marks and pauses\n",
        "4. Inflection deals with word forms and declensions\n",
        "5. Syntax manages sentence structure and word relationships\n",
        "6. Lexical errors cover word choice and usage\n",
        "7. Stylistic issues address tone and clarity\n",
        "\n",
        "The one_shot parameter adds an educational element - when enabled, it provides an example correction demonstrating proper error fixing. This example, \"pete sat hunced at the tble and was silent,\" shows multiple error types and their corrections, helping guide the model's approach to the task.\n",
        "\n",
        "The function then assembles these components based on which error types were requested. If the user specifies 'all', it includes every error category. Otherwise, it selectively includes only the requested error types, creating a focused correction lens.\n",
        "\n",
        "The prompt ends with clear instructions about preserving the original meaning while fixing errors. This constraint ensures that corrections improve the text's form without altering its substance.\n",
        "\n",
        "The final section handles the technical process of getting the model to generate corrections:\n",
        "- It packages the constructed prompt into a message format\n",
        "- Converts the text into tokens the model can process\n",
        "- Moves these tokens to the GPU for efficient processing\n",
        "- Generates the corrected text using the model's parameters\n",
        "- Decodes the model's output back into human-readable text\n",
        "\n",
        "The generation parameters are set for precision rather than creativity (do_sample=False), which is appropriate for a correction task where accuracy is paramount.\n",
        "\n",
        "This architecture creates a powerful tool for improving text quality, combining linguistic expertise with systematic error detection and correction. Its modular design allows for both comprehensive editing and targeted improvements to specific aspects of language use."
      ],
      "metadata": {
        "id": "_rZy3scYNeX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "NpSc6e9vN4qN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "36HpPKv7OtLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me walk you through this code, which sets up the critical components needed for processing and generating text with our language model. Let's break down each part to understand what's happening and why it matters.\n",
        "\n",
        "The first line loads a tokenizer matched to our specific model (microsoft/Phi-3-mini-4k-instruct in this case). A tokenizer is like a translator that converts human-readable text into numbers that the model can understand. Think of it as breaking down words into smaller pieces, similar to how we might break down \"sunflower\" into \"sun\" and \"flower\" to understand its meaning better.\n",
        "\n",
        "The next line addresses an important technical detail: setting the pad_token equal to the eos_token (end of sequence token). This is necessary because language models need a consistent way to handle texts of different lengths. Think of it like standardizing the end of sentences in different languages - some might use periods, others might use different marks, but we need one consistent way to mark the end.\n",
        "\n",
        "The final piece creates a TextStreamer object, which controls how the model's output is displayed. The skip_prompt=True parameter tells the streamer not to show the input text again in the output - similar to how when having a conversation, you don't repeat the question before giving your answer. The skip_special_tokens=True parameter removes internal markers that the model uses but aren't meant for human readers, like cleaning up stage directions before presenting a final theatrical script.\n",
        "\n",
        "Together, these components create a complete pipeline for text processing:\n",
        "1. The tokenizer converts human text into model-readable numbers\n",
        "2. The padding system ensures all inputs are properly formatted\n",
        "3. The streamer manages how the model's responses are presented back to humans\n",
        "\n",
        "This setup is fundamental to how the model understands and generates text, similar to how having proper tools and workspace organization is essential before starting any complex project. Each component plays a vital role in ensuring smooth communication between humans and the language model.\n",
        "\n",
        "Understanding this infrastructure helps us appreciate why the model can handle such sophisticated tasks as style transformation and error correction - it has the proper tools to break down, understand, and reconstruct text in meaningful ways."
      ],
      "metadata": {
        "id": "JWRsGN2iOvcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "      load_in_4bit=True,\n",
        "      bnb_4bit_compute_dtype = CFG.dtype\n",
        ")"
      ],
      "metadata": {
        "id": "HOS35wlnO3vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up a crucial configuration for running large language models efficiently with limited computational resources. Let me break down what's happening and why it matters.\n",
        "\n",
        "The BitsAndBytesConfig creates settings for quantization - a technique that reduces the precision of numbers used in the model to save memory while maintaining performance. Think of quantization like converting high-resolution photos to a lower resolution that still looks good but takes up less space.\n",
        "\n",
        "The configuration has two key settings:\n",
        "\n",
        "First, load_in_4bit=True tells the system to use 4-bit quantization. To understand why this is significant, let's consider how computers typically store numbers. Standard neural networks often use 32 or 16 bits per number, which provides high precision but requires lots of memory. By using just 4 bits, we can store numbers using only 1/8 or 1/4 of the original memory. This dramatic reduction is like compressing a large file so it can fit on a smaller drive.\n",
        "\n",
        "Second, bnb_4bit_compute_dtype = CFG.dtype (which we saw earlier was set to torch.bfloat16) specifies the precision to use during actual computations. This creates an interesting hybrid approach: while the model's weights are stored in 4-bit format to save memory, calculations are done in bfloat16 format to maintain accuracy. It's similar to how a photographer might store photos in a compressed format but temporarily decompress them for editing to ensure high-quality results.\n",
        "\n",
        "The bfloat16 format itself is worth understanding. Unlike standard 16-bit floating point numbers, bfloat16 maintains the same number of exponent bits as 32-bit numbers while reducing precision bits. This makes it particularly good for neural networks because it can represent a wide range of values (from very small to very large) while using less memory.\n",
        "\n",
        "This configuration represents a careful balance between three competing needs:\n",
        "1. Reducing memory usage to run large models on consumer hardware\n",
        "2. Maintaining enough numerical precision for the model to work effectively\n",
        "3. Keeping computation speed reasonable\n",
        "\n",
        "Without such optimizations, many modern language models would be too large to run on typical hardware. This configuration makes it possible to use sophisticated models on more modest hardware while still getting good results - democratizing access to advanced AI technology.\n",
        "\n",
        "The effectiveness of this approach shows how theoretical insights about neural networks (like their resilience to reduced precision) can be translated into practical engineering solutions that make advanced technology more accessible."
      ],
      "metadata": {
        "id": "OHk5WOpPO4I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(CFG.model,\n",
        "                                            torch_dtype = CFG.dtype,\n",
        "                                            quantization_config=quantization_config,\n",
        "                                            low_cpu_mem_usage = True\n",
        "                                            )\n",
        "\n",
        "model.generation_config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "qXeLMyv4O8Zd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b56cd47f561442ce8804ef96ef3a11da",
            "cf297d87e3c4438cacaf69045cfce7cf",
            "111487eabb8d47fd836770f87677aa05",
            "a960aee7cfe749fc843ad193b472ffd3",
            "217c1f756fa54baaa997e81d5a156bef",
            "7b77f3f9076241ccb9d81e6444251ee1",
            "533f5512c79f4ba98533c030a2758530",
            "41ab047a863549dfb25945df889f2719",
            "dda19ad57d994b6fb2759add5ab619ad",
            "343d84d8727341709700d96b31af2917",
            "0abafeab9e9940869b1feb66dd8c7500"
          ]
        },
        "outputId": "ac027853-11d0-405f-c2f5-46378af44b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b56cd47f561442ce8804ef96ef3a11da"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code loads and configures a large language model, setting it up to run efficiently while maintaining good performance. Let me break down each part to help build a clear understanding of what's happening.\n",
        "\n",
        "The main function, AutoModelForCausalLM.from_pretrained(), loads our chosen model. The \"ForCausalLM\" part of the name tells us this is specifically designed for text generation, where each word is predicted based on all the previous words - similar to how we naturally complete sentences based on their beginnings.\n",
        "\n",
        "Let's examine each configuration parameter:\n",
        "\n",
        "torch_dtype = CFG.dtype sets the numerical format to bfloat16, which we discussed earlier. This choice reflects a deep understanding of how neural networks process numbers. While regular computers often need high precision for calculations like banking, neural networks can work remarkably well with less precise numbers. Think of it like how human brains can recognize a friend's face whether it's in bright sunlight or dim evening light - we don't need perfect precision to perform complex tasks.\n",
        "\n",
        "quantization_config=quantization_config applies our earlier 4-bit quantization settings. This is where the model performs its memory diet, converting its internal weights from high-precision numbers to more compact 4-bit representations. Imagine converting a high-resolution photo to a smaller size that still captures all important details - we're doing the same thing with the model's knowledge.\n",
        "\n",
        "low_cpu_mem_usage=True tells the system to be extra careful with memory during the loading process. This is particularly important when working with large models. It's similar to how a moving company might carefully plan how to get large furniture through a narrow doorway - we're making sure the model loads efficiently even with limited resources.\n",
        "\n",
        "The final line, setting pad_token_id, ensures consistency between our tokenizer and model. This might seem like a small detail, but it's crucial for proper text processing. Think of it like making sure two people working together use the same signal to indicate they've finished speaking - without this coordination, communication breaks down.\n",
        "\n",
        "This configuration represents years of research and engineering advances in making large language models practical. Just a few years ago, running models of this size would have required expensive specialized hardware. Now, through techniques like quantization and careful memory management, we can run sophisticated models on more modest hardware while maintaining most of their capabilities.\n",
        "\n",
        "Understanding these settings helps us appreciate the careful balance between model performance, memory usage, and computational efficiency. It's like conducting an orchestra - each setting must be tuned just right to create the desired result. The choices made here reflect a deep understanding of both the theoretical foundations of neural networks and the practical constraints of running them on real hardware."
      ],
      "metadata": {
        "id": "cP3CjUq8O9PM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "email = \"\"\"\n",
        "Dear Mr Thomson\n",
        "I am writing to gave you an update about the progress of our project. During last two weeks, our team have made significant improvements in terms of the developments of the new software platform.\n",
        "I would like too schedule a meeting next week to discuss the results what we achieved. The meeting will took approximately 2 hours, and we need discussing the following topics:\n",
        "\n",
        "Implementation of new features which was requested by clients\n",
        "Budget adjustments for the next quater\n",
        "Time line for deploying the system\n",
        "\n",
        "Please let me known what time suits you better. I am available between monday and thursday next week, preferable in the morning hours.\n",
        "Looking forward to hear from you soon.\n",
        "Best Regards\n",
        "Sarah Miller\n",
        "Senior Project Manger\n",
        "Digital Solutions Department\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "t9hRJWViOZSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_corrected = resolve_mistakes(email, options = ['punctuation'], one_shot = True)"
      ],
      "metadata": {
        "id": "GfZ8rw3zObIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7b641c-1739-4648-a0d6-28ef7685af82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Mr. Thomson,\n",
            "\n",
            "I am writing to provide you with an update on the progress of our project. Over the past two weeks, our team has made significant improvements in terms of the development of the new software platform.\n",
            "\n",
            "I would like to schedule a meeting next week to discuss the results we have achieved. The meeting will take approximately two hours, and we need to cover the following topics:\n",
            "\n",
            "* Implementation of new features requested by clients\n",
            "* Budget adjustments for the next quarter\n",
            "* Time line for deploying the system\n",
            "\n",
            "Please let me know what time works best for you. I am available between Monday and Thursday next week, preferably in the morning hours.\n",
            "\n",
            "Looking forward to hearing from you soon.\n",
            "\n",
            "Best Regards,\n",
            "Sarah Miller\n",
            "Senior Project Manager\n",
            "Digital Solutions Department\n"
          ]
        }
      ]
    }
  ]
}